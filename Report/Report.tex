\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{float}
\usepackage{verbatim}
\usepackage[demo]{graphicx}
\usepackage{gensymb}
\usepackage{floatrow}
\usepackage{hyperref}
\hypersetup{linktoc=all}
\usepackage{blindtext}
\usepackage{wrapfig}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{epstopdf}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{enumitem}
\usepackage{steinmetz}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage[utf8]{inputenc}
\usepackage{fourier} 
\usepackage{array}
\usepackage{makecell}
\usepackage{caption}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Multi-Object tracking and Trajectory Prediction for Autonomous Vehicles\\
}

\author{\IEEEauthorblockN{Vinit Awale}
  \IEEEauthorblockA{
    \textit{Department of Electrical Engineering} \\
    \textit{Indian Institute of Technology, Bombay}\\
    \textit{Mumbai, India} \\
    awale.vinit@gmail.com}
  \and
  \IEEEauthorblockN{Prof. Naveen Arulselvan}
  \IEEEauthorblockA{\textit{AI \& Robotics Technology Park} \\
    \textit{ARTPARK, IISC Bangalore}\\
    Karnataka, India \\
  }
}

\maketitle

\begin{abstract}
  This document specifies the implementation details of the perception module using cameras of a self-driving car. This project was completed as a part of the Summer Internship program at ARTPARK, IISC Bangalore, under the mentorship of Prof. Naveen Arulselvan. The perception module consists of multi-object detection, tracking and trajectory prediction. The multi-object detection is based on the \textit{YOLO v5} algorithm{} \cite{YOLOv5}. We have used the \textit{ Deep Sort} algorithm for multi-object tracking based on the paper "Simple online and realtime tracking with a deep association metric". The trajectory prediction module is implemented using \textit{PEC Net} based on the paper "It is not the journey but the destination: Endpoint conditioned trajectory prediction". Python and PyTorch framework have been used for the code implementation.
\end{abstract}

\begin{IEEEkeywords}
  Object detection, multi-object tracking, trajectory prediction, YOLOv5, Deep Sort, PEC Net, Autonomous vehicles
\end{IEEEkeywords}

\section{Introduction}
Perception is a central problem for any autonomous agent, be it humans, robots or self-driving vehicles. This module helps for a smoother and more reliable control of the car using the path-planning module of the autonomous agent. It can also aid in pose estimation. For our project, we have included the following sub-modules for the perception:
\begin{itemize}
  \item Multi-object detection using the \textit{YOLOv5} algorithm.
  \item Multi-object tracking using the \textit{Deep Sort} algorithm.
  \item Trajectory prediction using the \textit{PEC Net} algorithm.
\end{itemize} \\
Object detection in the context of autonomous driving refers to detecting the objects present in the scene (making use of the camera sensors on the autonomous vehicle) by making bounding boxes surrounding the detected objects. This is followed by identifying the class of the objects. The family of YOLO (You Only Look Once) models are the most popular object detection models for autonomous driving. The YOLOv5 model is a state-of-the-art object detection model that is capable of detecting 80 classes of objects. The model is trained on the MS COCO dataset, containing over 1.2 million images and over 20,000 bounding boxes for the 80 classes of objects. \\
Multi-object tracking refers to the problem of tracking the objects detected across frames. For this project, we are implementing the \textit{Deep Sort} algorithm for tracking the bounding boxes. Simple Online Realtime Tracking \textit{SORT} is an approach of multi-object tracking using simple and effective algorithms such as Kalman Filter. Including an association metric (using deep learning) for the detected object across frames leads to a more robust and accurate multi-object tracking called Deep Sort. \\
The problem of trajectory prediction (as is already apparent from the name) involves predicting the agents detected by the YOLOv5 model. PEC Net uses an encoder-decoder architecture for predicting the agents detected by the YOLOv5 model. The encoder is a neural network that encodes the input image into a vector of fixed size. The decoder is a fully connected neural network that decodes the vector into a high-dimensional vector. For the implementation of PEC Net, we need an aerial view of the scene. However, we are working with datasets of camera images taken in the ego-centric view in our implementation. Hence for a complete end-to-end perception pipeline, we need to move the detections of our object detection module to birds-eye view. This is a part of future work. \\
The code for the project is available at %<enter link>%

\section{Background and Related Works}
\subsection{Object Detection}
The problem of object detection and object tracking is quite well known in the field of computer vision. Modern Object detectors are composed of two main components:
\begin{itemize}
  \item The backbone: This is the part of the detector that is responsible for extracting the features of the objects in the image using a deep convolutional neural network.
  \item The head: This is the part of the detector that is responsible for classifying the objects in the image.
\end{itemize}

On the basis of head component, we have two main approaches for object detection:
\begin{itemize}
  \item Two stage Object Detector: This approach leads to high localization and object detection accuracy of the results. \\
        Example: R-CNN, fast R-CNN, faster R-CNN, R-FCN, Libra R-CNN, etc
  \item One stage Object Detector: This approach leads to a higher reference speed. \\
        Example: SSD, YOLO, RetinaNet, etc
\end{itemize}

We have chosen the One-stage Object Detector approach since we need a higher reference speed for autonomous driving applications. Specifically, we have used YOLO detection for our application.

\subsection{Multi-Object Tracking}
Multi-object tracking methods usually include a tracking network and a detection network. The tracking network is responsible for updating the position and orientation of the object in the scene. The detection network is responsible for detecting the object in the scene. The tracking network is responsible for the data association across the frames.  \\
The detections from the object detector can be processed in one of the following ways:
\begin{itemize}
  \item Batch method: In this approach, the data association problem is solved by storing the image frames with detections in a batch. The batch is then processed one by one. \\
  \item Online method: In this approach, the data association problem is solved by processing an image frame with a detection one at a time. \\
\end{itemize}

Practically, the batch method performs better than online methods for the tracking network. However, the batch method is computationally expensive, which means that the tracking network needs to process the entire batch of detections before starting the next frame. Hence, this approach is not viable for our project. Therefore, we have chosen the online method for our project. \\
Famous object tracking methods include:\\
KCF tracker, SORT tracker, MOSSE, KCF+SORT, MOSSE+KCF, MOSSE+KCF+SORT, MedianFlow, etc. \\

For our project, we have chosen the Deep SORT, an online multi-object tracking algorithm that incorporates a deep association metric for the tracking network.

\subsection{Trajectory Prediction}


\section{Datasets}
Autonomous driving is an emerging field; hence there are limited datasets that can be used for training and testing which would resemble the real world. The datasets that we have used for our project are:
\begin{itemize}
  \item Lyft level 5 Prediction dataset \cite{ Lyft} \\
        This is a self-driving dataset for motion prediction, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time.
  \item KITTI dataset \cite{KITTI} \\
        The dataset comprises of Raw (unsynced+unrectified) and processed (synced+rectified) color stereo sequences (0.5 Megapixels, stored in png format), captured and synchronized at 10 Hz. This dataset was only used for testing the perception modules of our project. \\
        Along with camera sequences, the dataset also contains 3D Velodyne point clouds, 3D GPS/IMU data and 3D object tracklet labels. All of these will be useful for the proposed future works of the project. \\
\end{itemize}
\section{Procedure, Experiments and Results}
\subsection{Object detection}
\subsubsection{\textbf{YOLO algorithm}}
YOLO is an abbreviation for the term ‘You Only Look Once’. This is an algorithm that detects and recognizes various objects in a picture (in real-time). Object detection in YOLO is done as a regression problem and provides the class probabilities of the detected images. YOLO algorithm employs convolutional neural networks (CNN) to detect objects in real-time. As the name suggests, the algorithm requires only a single forward propagation through a neural network to detect objects.
\subsubsection*{\textbf{Working of YOLO}}
In this subsection we will discuss the working of YOLO algorithm in brief.
\begin{itemize}
  \item First split the image into smaller S x S grid cells. This S is a hyperparameter that can be tuned to improve the performance of the algorithm. However, the larger the S, the more computationally expensive the algorithm will be but the results will be more accurate. In the original paper, the S is set to 7 (as shown in the following figure).
        \begin{figure}[H]
          \centering
          \includegraphics[width=0.95\linewidth]{Images/yolo_1.png}
        \end{figure}
  \item For each grid cell make predictions of B bounding boxes (xcentre ,ycentre ,width, height) and confidence for each bounding box (P[object]). The value of B was set to 2 in the original paper. Here, multiple bounding boxes are predicted for each grid cell. This helps in the case when a single grid cell has the center of multiple objects. However, as the number of grid cells (B) increases, the probability that multiple objects are detected in a single grid cell decreases.
        \begin{figure}[H]
          \centering
          \includegraphics[width=0.95\linewidth]{Images/yolo_2.png}
        \end{figure}
  \item Along with this the model also predicts conditional class probability (out of the C classes) i.e. $P[class_i|object]$ for each grid cell.
        \begin{figure}[H]
          \centering
          \includegraphics[width=0.95\linewidth]{Images/yolo_3.png}
        \end{figure}
  \item The model then predicts the class label for each bounding box using Bayes Theorem
        \begin{equation*}
          P[class_i] = P[class_i|object] * P[object]
        \end{equation*}
        \begin{figure}[H]
          \centering
          \includegraphics[width=0.5\linewidth]{Images/yolo_4.png}
        \end{figure}
\end{itemize}
From the above image we can see that there are several bounding boxes detected for each class. Now, to get rid of these bounding boxes, we need to apply non-maximal suppression.
\subsubsection*{\textbf{Non-maximal suppression}}
The steps involved in applying non-maximal suppression are as follows:
\begin{itemize}
  \item Find the IoU (Intersection over Union) for all the bounding boxes of the same class.
        \begin{figure}[H]
          \centering
          \includegraphics[width=0.5\linewidth]{Images/iou.png}
        \end{figure}
  \item Keep the bounding box  with maximum class
        probability and discard other bounding boxes with IoU > threshold.
\end{itemize}
Hence, using this we get rid of the multiple bounding boxes of the same class and only preserve the one with the highest class probability. Finally, the result we obtain is shown in the following figure.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{Images/yolo_5.png}
\end{figure}

\subsubsection*{\textbf{Architecture of YOLO}}
The architecture of YOLO is shown in the following figure.
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Images/YOLO_architecture.png}
\end{figure}

The architecture of YOLO comprises of 24 convolutional layers followed by 2 fully connected layers. In the architecture, alternating $1\times 1$  convolutional layers reduce the feature space from the preciding layers. In the original paper the architecture  was trained on the ImageNet dataset with all the images resized to 224 x 224 pixels.

\subsubsection*{\textbf{Loss Function}}
The loss function used while training the YOLO model is as follows:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{Images/yolo_loss.png}
\end{figure}
where, $\mathbb{1} _{i}^{obj}$  denotes if object appears in cell i and $\mathbb{1} _{ij}^{obj}$ denotes that the jth bounding box predictor in cell i is “responsible” for that prediction.  \\
The first two terms in the loss function correspond to the localization loss. The next two terms correspond to the confidence loss. The last term is the classification loss.

\subsubsection{\textbf{YOLO v5}}
For our project we have used pretrained YOLOv5 model, which is an improve version of the basic YOLO architecture. Since, we decided to implement the entire module using PyTorch, we decided to change the object detection sub module to YOLOv5 written in PyTorch from YOLOv3 which has been implemented using the Darknet framework. Also, according to various studies \cite{YOLOv5comparision} regarding the comparison of the two models, it has been found that YOLO v5 is the most optimal model for deployment due to its speed and accuracy.
Following is the result of one frame on the Lyft dataset when the YOLOv5 algorithm was implemented on it.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{Images/YOLOv5_ip.png}
  \caption{Original camera frame from the Lyft Level 5 dataset}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{Images/YOLOv5_op.png}
  \caption{Result of YOLO v5 object detection}
\end{figure}

When implemented on the KITTI dataset, the results we as follows:
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Images/YOLOv5_Kitti_ip.png}
  \caption{Original camera frame from the KITTI dataset}

\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Images/YOLOv5_Kitti_op.png}
  \caption{Result of YOLO v5 object detection}
\end{figure}

\subsubsection*{\textbf{FPS obtained}}
We tested the Object detection module on KITTI dataset and the Lyft Level 5 dataset. Also, two GPUs were used for testing. One being Nvidia Geforce RTX 3060 (mobile) GPU and the other being Nvidia Tesla T4 GPU (server). The following table shows the FPS obtained on the KITTI dataset.
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{Images/YOLO_fps.png}
\end{figure}

\subsection{Multi-Object Tracking}
For this project, we have implemented Deep SORT algorithm for multi-object tracking. The algorithm is based on the paper \cite{Deep_sort} which is a state-of-the-art algorithm for multi-object tracking. Now, let us look at the working of the algorithm.

\subsubsection*{\textbf{Working Deep Sort}}
The tasks included in the implementation of Deep SORT are as follows:\\
\begin{itemize}
\item OBJECT DETECTION \\
We have already implemented YOLO v5 algorithm for the object detection tasks


\item ESTIMATION OF THE BOUNDING BOXES OF THE OBJECTS IN IMAGE SPACE \\
We assume a very general tracking scenario where the camera is uncalibrated and where we have no ego-motion information available. We use a Standard Kalman Filter with constant velocity motion and linear observation model on a eight dimensional space $(x,y,\gamma, h, \dot{x}, \dot{y}, \dot{\gamma}, \dot{h}) $

\item ASSIGNMENT PROBLEM\\
We solve the association problem using the Hungarian Algorithm. For that we need to form a cost matrix using an association metric. Deep Sort incorporates two association metrics.
\begin{itemize}
\item Motion Information \\
To incorporate the motion information, we use the Mahalanobis distance between the predicted and the newly measured bounding boxes.
\begin{equation*}
d^1(i,j) = (d_j - y_i)^T \Sigma^{-1} (d_j - y_i)
\end{equation*}
where, $d_j$ is the predicted bounding box and $y_i$ is the newly measured bounding box. The $\Sigma$ matrix is the covariance matrix of the motion model. The Mahalanobis distance is a robust distance metric in the case when there is correlation between the n-dimensional vectors.




\section{Limitations}

\section{Future work}

\section{Acknowledgment}

\bibliographystyle{plain}
\bibliography{cite.bib}


\end{document}
